
Welcome to this course on compilers. My
name is Alex Aiken. I'm a professor here
in Stanford University and we're going to
be talking about the implementation of
programming languages. There are two major
approaches to implementing programming
languages, compilers, and interpreters.
Now this class is mostly about compilers
but I do want to say a few words about
interpreters here in the first lecture.
So, what does an interpreter do? Well, I'm
gonna draw a picture here. This box is the
interpreter here, let me label it with a
big I. It takes as input your program.
That you wrote and whatever data that you
want to run the program on, and it
produces the output directly. Meaning that
It doesn't do any processing in the
program before it executes it on the
input, So you just write the program and
you invoke the interpreter on the date and
the program immediately begins running. So
we can say that the interpreter is online.
Meaning the work that it does is all part
of running your program. And now compiler
is structured differently. So we can draw
a picture here, which we'll label with a
big "C" for the compiler. And the compiler
takes as input trust your program. And
then it produces an executable. And this
executable is another program. It might be
assembly language, it might be byte code.
It could be in any number of different
implementation languages. But now this can
be run separately on your data. And that
will produce the output. Okay. And so in
this structure the compiler is off line,
Meaning that we preprocess the program
first. A compiler is essentially a
preprocessing step that produces the
executable. And then we can run that same
executable on many, many different,
inputs, on many different data sets,
without having to recompile or do any,
other processing of the program. I think
it's helpful that I give a little bit of
history about how compilers and
interpreters were first developed. So the
story begins in the 1950s And in
particular, with a machine called the 704,
built by IBM. This was their first co
mmercially successful machine, although
there had been some earlier machines that
they had tried out. But anyway, the
interesting thing about the 704, well once
customers started buying it and using it,
is that they found that the software costs
exceeded the hardware costs. And not just
by a little bit, but by a lot. And. This
is important, because, these, the hardware
in these, in those days was extremely
expensive. And, even then, when hardware,
cost the most in absolute and relative
terms, more than it would ever cost again.
Already, the software was the dominant,
expense, in, in making good use out of
computers. And this led a number of
people, to think about how they could do a
better job of writing software. How they
could make, programming more productive.
One of the earliest efforts to improve the
productivity of program was called speed
coding Developed in 1953 by John Backus.
Decoding is what we would call today an
early example of an interpreter. And like
all interpreters it has some advantages
and disadvantages. The primary advantage
was that it was much faster to develop the
program, So the, in that sense, the
programmer was much more productive. But
among its disadvantages code written
decode written [inaudible] were ten to
twenty times slower. Than handwritten
programs and that?s also true of
interpreted programs today. So if you have
an implementation that uses an
interpreter, the interpreter would be much
slower than either a compiler or a writing
code by hand. And also, the speed code
interpreter took up 300 bytes of memory.
And that doesn't seem like very much, in
fact 300 bites today would seem like an
incredibly tiny program but in those days
you have to keep in mind that this was 30
Percent Of the, memory on the machine. So
this was 30 percent of the entire memory
of the 704. And so the amount of space
that the interpreter took up was itself a
concern. Now, speed coding did not become
popular, but John [inaudible] thought it
was promising. And it gave him an idea for
another project. The mo st important
applications in those days were scientific
computations. And programmers thought in
terms of writing down formulas in a form
that the machine could execute. John
thought that the problem with speed coding
was that the formulas were in fact
interpreted. And he thought if first the
formulas were translated into a form that
the machine could execute directly. That
the code will be faster, And while still
allowing the programmer to write the, the,
programs at a high level, And thus was the
formula translation project, or FORTRAN
project born. Now, FORTRAN ran from 1954
To 1957, And interestingly, they thought
it would only take them one year to build
the compiler, but it wound up taking
three. So, just like today, they weren't
very good at predicting how long software
projects would take. But it was a very
successful project. By 1958, over 50
percent of all code was written in
FORTRAN. So 50 percent of programs were in
FORTRAN. And that is very rapid adoption
of a new technology. We would be happy
with that kind of success today. And, of
course, at that time, they were ecstatic.
And everybody thought that, FORTRAN both
raised the level of abstraction, improved
programmer productivity, And allowed
everyone to make much better use of these
machines. So FORTRAN one was the first
successful high level language and it had
a huge impact on computer science. In
particular, it led to an enormous body of
theoretical work. And one of the
interesting things about programming
languages, actually, is the, combination
of theory. And practice because it's not
really possible, in programming languages,
to do a good job without having both a, a
very good grasp of fairly deep theory and
also good engineering skills. So there's a
lot of very good systems building material
in programming languages and typically
involves a very subtle and, fruitful
interactions with theory. So, and this is
one of the things that I think is the most
attractive about theory as a subject of
studying computer science. And the impact
of FORTRAN was not just on computer
science research, of course, But also on
the development of, practical compilers.
And, in fact, its influence was so
profound, that today, auto compilers still
preserve the outlines of FORTRAN one. So
what, what is the structure of FORTRAN
one? Well, it consists of five phases,
Lexical analysis and parsing. Which
together, take care of the syntactic
aspects of the language Semantic analysis,
which, of course, takes care of more
semantic aspects, Things like types and
scope rules. Optimization, which is a
collection of transformations on the
program, to either make it run faster, or
use less memory. And finally code
generation, which actually does the
translation to another language, and
depending on our goals, that translation
might be to machine code, it might be to a
byte code for a virtual machine, or it
might even be to another high level
programming language. Well, that's it for
this lecture, and next time we'll pick up
here and talk about these five phases in
more detail.
