
Welcome to this course on compilers. My
name is Alex Aiken. I'm a professor here
at Stanford University. And we're going to
be talking about the implementation of
programming languages. >> There are two
major approaches to implementing
programming languages, compilers, and
interpreters. Now, this class is mostly
about compilers. But, I do want to say a
few words about interpreters here in the
first lecture. So, what does an
interpreter do? Well, I'm gonna draw a
picture here, this box is the interpreter,
and it takes, let me label it with a big
I, it takes as input, your program. That
you wrote, And, whatever data that you
want to run the program on. And it
produces the output directly. Meaning that
it doesn't do any processing of the
program before it executes it on the
input, So you just write the program, and
you invoke the interpreter on the data,
and the program immediately begins
running. And so, we can say that the
interpreter is, is online, meaning it the
work that it does is all part of running
your program. Now a compiler is structured
differently. So, we can draw a picture
here. Which we'll label with a big C, for
the compiler, And the compiler takes as
input just your program. And then it
produces an executable. And this
executable is another program, might be
assembly language, it might be bytecode.
It could be in any number of different
implementation languages. But now this can
be run separately on your data. And that
will produce the output. Okay? And so in
this structure the compiler is offline,
Meaning that we pre-process the program
first. The compiler is essentially a
pre-processing step that produces the
executable, and then we can run that same
executable on many, many different inputs,
on many different data sets without having
to recompile or do any other processing of
the program. I think it's helpful to give
a little bit of history about how
compilers and interpreters were first
developed. So the story begins in the
1950s and in particular with a machine
called the 704 built by IBM. Thi s was
their first commercially successful
machine, although there had been some
earlier machines that they had tried out.
But anyway the interesting thing about the
704 well, once customers started buying it
and using it, is that they found that the
software costs exceeded the hardware
costs. And not just by a little bit, but
by a lot And, This is important because
these, the hardware in these, those days
was extremely expensive. And even then
when hardware cost the most in absolute
and relative terms, more than they would
ever cost again, already the software was
the dominant expense in, in making good
use out of computers. And this led a
number of people to think about how they
could do a better job of writing software.
How they could make programming more
productive. Where the earliest efforts to
improve the productivity of programming
was called speed coding, developed in 1953
by John Backus. Now, speed coding is what
we call today, an early example of an
interpreter. And like all interpreters, it
had some advantages and disadvantages. The
primary advantage was that it was much
faster, to develop the programs. So the,
in that sense, the programmer was much
more productive, But among its
disadvantages, code written, speed code
programs were ten to twenty times slower.
Then handwritten programs and that's also
true of interpreted programs today. So if
you have an implementation that uses an
interpreter, they're going to be much
slower than either a compiler or writing
code by hand. And also, the speed code
interpreter took up, 300 bytes of memory.
And that doesn't seem like very much. In
fact, 300 bytes, today, would seem like an
incredibly tiny, program. But in those
days, you have to keep in mind, that this
was 30 Percent Of the memory on the
machine. So this was 30 percent of the
entire memory of the 704. And so the
amount of space that the interpreter took
up was itself a concern. Now speed coding
did not become popular, but John Backus
thought it was promising and it gave him
the idea for another project. The most
important applications in those days were
scientific computations, and programmers
thought in terms of writing down formulas
in a form that the machine could execute.
John thought that the problem with speed
coding was that the formulas were in fact
interpreted and he thought if first the
formulas were translated in to a form that
the machine could execute directly. That
the code would be faster, And while still
allowing the programmer to write the, the,
the programs at a high level, and thus was
the Formula Translation Project or FORTRAN
Project born. Now FORTRAN ran from 1954 To
1957, And interestingly, they thought it
would only take them one year to build the
compiler but it would end up taking three.
So just like today, they weren't very good
at predicting how long software projects
would take. But it was a very successful
project. By 1958, over 50 percent of all
code was written in FORTRAN. So 50 percent
of programs were in FORTRAN, And, that is
very rapid adoption of a new technology.
We would be happy with that kind of
success today, and of course at that time
they were ecstatic, And everybody thought
that FORTRAN both raised the level of
extraction, improved programmer
productivity, and allowed everyone to make
much better use of these machines. So
FORTRAN one was the first successful high
level language and it had a huge impact on
computer science. In particular, it led to
an enormous body of theoretical work. And
one of the interesting things about
programming languages, actually, is the
combination of theory. And practice
because it's not really possible in
programming languages to do a good job
without having both a, a very good grasp
of fairly deep theory and also good
engineering skills. So there's a lot of
very good systems building material in
programming languages and typically it
involves a very subtle and fruitful
interaction with theory. And so, and this
is one of the things, I think, that's most
attractive about the area's the subject of
studying computer science. And the impact
of FORTRAN was not just on computer
science research, of course, but also on
the development of, practical compilers.
And, in fact, its influence was so
profound, that today, auto compilers still
preserve the outlines of FORTRAN one. So
what was the structure of [inaudible]?
Well it consists five phases lexical
analysis and parsing, which together take
care of the syntactic aspects of the
language, semantic analysis, which, of
course, takes care of more semantic
aspects, things like types and scope
rules. Optimization, Which is a collection
of transformations on the program to
either make it run faster or use less
memory. And finally code generation which
actually does the translation to another
generation. And depending on our goals,
that translation might be to machine
codes. It might be to a bite code for a
virtual machine. It might be to another
high level programming language. Well
that's it for this lecture, and next time
we'll pick up here and talk about these
five phases in more detail.
